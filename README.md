![Banner](images/banner.png)


## üßë‚Äçüíº Jason A. Ballard  
**Instructional Systems Special | Data Scientist | Data and AI Officer | Data Literacy Advocate | Educator in Professional Military Education**

Welcome! I'm Jason A. Ballard, an experienced data and AI integration leader currently serving as a Data and AI Officer for the **US Army Combined Arms Center** at Fort Leavenworth, Kansas. My work bridges data science, AI strategy, and higher education, focusing on transforming decision-making through data literacy and innovation.

I invite you to explore my GitHub repository [jbtallgrass](https://github.com/JBtallgrass?tab=repositories), where I share insights, tools, and resources geared toward data literacy and advanced analytics in educational contexts. My projects emphasize practical solutions, open collaboration, and a commitment to enhancing data accessibility across teams.

### üîë Key Areas of Focus:
- **Data Strategy & Governance**: Developing frameworks that promote data-driven decision-making and cross-departmental data sharing.  
- **AI & Analytics**: Leveraging data analytics and GenAI to unlock insights and drive transformational initiatives within Army University.  
- **Data Literacy & Education**: Equipping leaders and students with data literacy skills critical for today's complex, data-rich environments.  

Please don't hesitate to connect, collaborate, or contact me if our interests align. **Let's make data-driven transformation a reality together.**  

üìç **LinkedIn**: [Jason A. Ballard](https://www.linkedin.com/in/jasonaballard)

üìç **GitHub**: [jbtallgrass](https://github.com/JBtallgrass)
---
# üåä Rafting Feedback Streaming Project (Module 5)

---

## üìö Table of Contents  
- [Project Overview](#project-overview)  
- [Technologies Used](#technologies-used)  
- [Setup & Requirements](#setup--requirements)  
- [Project Components](#project-components)  
- [Workflow](#workflow)  
- [Project Structure](#project-structure)  
- [Commands Reference](#commands-reference)  

---

### üìå Project Overview  

This project demonstrates how to build a **streaming analytics pipeline** using **Apache Kafka**, **Python**, and a **relational database (SQLite)** for data storage. It simulates real-time data generation, ingestion, and processing while storing processed data in a structured database for further analysis and integration with BI tools.  

---
## üåä Rafting Feedback Streaming Project Description

This project is designed to **stream, process, and analyze real-time customer feedback** from rafting trips on the **French Broad River, NC**, using **Apache Kafka**. It integrates customer reviews with **weather and river flow conditions**, providing valuable insights into trip experiences and environmental impacts.

### ‚ö†Ô∏è Note: ‚ö†Ô∏è
The data in this project is **fictitious** with the use of  **Generative AI (GenAI)** assistants to **generate, problem-solve, and debug** the process.

---

### üéØ Goals  
- **Real-time processing** of structured (CSV) and semi-structured (JSON) data.  
- **Automated enrichment** of feedback with weather and river conditions.  
- **Performance tracking** for rafting guides based on customer reviews.  
- **Predictive insights** into trip satisfaction and environmental impact.  

### üö£ Data Sources  
- **Customer Feedback**: Reviews from rafting trip participants.  
- **Weather Conditions**: Temperature, wind speed, and precipitation.  
- **River Flow Levels**: Water level, current speed, and temperature.  

### ‚ö° Technologies Used  
- **Apache Kafka**: Real-time message streaming and processing.  
- **Python**: Data generation, transformation, and analytics.  
- **dotenv**: Environment variable management.  
- **Loguru**: Logging feedback and performance.  
- **matplotlib**: Data visualization for performance trends.  
- **Pandas**: Data manipulation and analysis.  
- **VS Code**: Development environment.  
- **SQLite**: Relational database for storing processed data.  

### üîë Key Features  
- **Real-time data ingestion** from a Kafka topic or a live data file.  
- **Data storage and processing** in SQLite, demonstrating integration with relational databases.  
- **Multiple consumer options**: File-based consumer for testing or Kafka-based consumer for real-time processing.  

---

## üõ†Ô∏è Setup & Requirements

### ‚úÖ Prerequisites
- **Python 3.11+**
- **Kafka & Zookeeper** installed and running.
    - bin/zookeeper-server-start.sh config/zookeeper.properties
    - bin/kafka-server-start.sh config/server.properties
  - **Virtual Environment** set up for dependency management.

### üì• Installation and Setup

1. Clone the project:
   
2. Create and activate a virtual environment:
   
3. Install dependencies:
  
4. Set up Kafka and Zookeeper:
   Follow the instructions in [Kafka Install Guide](Jballard_docs/kafka-install-guide.md).

5. Configure environment variables in `.env`:
   ```
   ZOOKEEPER_ADDRESS=172.30.179.152:2181
   KAFKA_BROKER_ADDRESS=172.30.179.152:9092
   KAFKA_CONNECTION_TIMEOUT=30000
   RAFTING_TOPIC=rafting_feedback
   RAFTING_INTERVAL_SECONDS=2
   ```
---

## üîπ Project Components

### 1. Data Generation
- **Weather Data** (`utils_generate_weather_data.py`): Generates synthetic weather data for the rafting region.
- **River Flow Data** (`utils_generate_river_flow.py`): Creates realistic river flow conditions.
- **Rafting Feedback** (`utils_generate_rafting_data.py`): Produces customer reviews with a mix of positive and negative feedback.

### 2. Kafka Producers and Consumers
- **Rafting Producer (`rafting_producer.py`)**: Streams generated feedback data to the `rafting_feedback` topic.
- **JSON Consumer (`rafting_consumer.py`)**: Logs all feedback, flags negative comments, and enriches messages with weather and river data.
- **CSV Consumer (`csv_rafting_consumer.py`)**: Processes JSON feedback and republishes it as CSV-friendly structured data.
- **CSV Feedback Consumer (`csv_feedback_consumer.py`)**: Writes structured feedback to a CSV file for analysis.
- **Processed CSV Producer (`csv_rafting_producer.py`)**: Enhances and republishes CSV feedback with status flags and trip disruption alerts.

### 3. Visualization
- **Real-Time Feedback Charts (`jb_project_consumer.py`)**:
  - Positive vs. Negative feedback.
  - Weekly performance trends.
  - Weather impact on feedback.
  - River flow and feedback correlation.

---

## üîÑ Workflow

Here‚Äôs the workflow for the project:
**Data Generation**: The producer (rafting_producer.py) simulates real-time data, streaming messages to a Kafka topic or saving them to a live data file.

**Real-Time Feedback Processing**: Consumers read messages from the Kafka topic or process, enrich the data (e.g., adding weather and river conditions), and store the processed results in the SQLite database for persistence and further analysis.

**Data Storage**: SQLite serves as the data store for processed data, enabling historical reference, integration with BI tools, and deeper insights into rafting feedback.

**Visualization**: jb_project_consumer.py updates real-time charts every 10 messages, visualizing positive vs. negative feedback, weather impact on feedback, and river flow correlations.
---

## üìä Visualizations

- **Positive vs. Negative Feedback (Bar Chart)** 
- **Weekly Feedback Trends (Line Chart)**
- **Weather vs. Negative Feedback (Bar Chart)**
- **River Flow vs. Feedback Type (Box Plot)**

---

## üìÇ Project Structure

```
‚îú‚îÄ‚îÄ data/                  # Generated data files
‚îú‚îÄ‚îÄ images/                # Visualization charts
‚îú‚îÄ‚îÄ utils/                 # Utility scripts for data generation and logging
‚îú‚îÄ‚îÄ producers/             # Kafka producers
‚îú‚îÄ‚îÄ consumers/             # Kafka consumers
‚îú‚îÄ‚îÄ .env                   # Environment variables
‚îú‚îÄ‚îÄ requirements.txt       # Project dependencies
‚îî‚îÄ‚îÄ README.md              # Project documentation
```
---
### üìä Final Overview: The Full Data Flow

  Zookeeper -->|Manages Brokers| --> KafkaBroker;

  KafkaBroker -->|Creates Topics|  --> Rafting_Producer;

  Rafting_Producer -->|Sends Messages|  --> Rafting_Feedback;

  Rafting_Feedback -->|Consumes Messages|  --> jb_Rafting_Consumer;

  jb_Rafting_Consumer -->|Stores in SQLite|  --> SQLite_DB;

  Rafting_Feedback -->|Processes & Republishes|  --> csv_Rafting_Consumer;

  csv_Rafting_Consumer -->|Sends Structured Data|  --> Rafting_CSV_Feedback;

  Rafting_CSV_Feedback -->|Consumes & Stores in SQLite|  --> csv_Feedback_Consumer;

  Rafting_CSV_Feedback -->|Processes & Republishes|  --> csv_Rafting_Producer;

  csv_Rafting_Producer -->|Sends Processed Data|  --> Processed_CSV_Feedback;

  wx_Producer -->|Generates Weather Data|  --> Weather_JSON;

  Weather_JSON -->|Consumes & Visualizes|  --> wx_Consumer;

---

## ‚ö†Ô∏è Important Notes

1. Ensure Kafka and Zookeeper are running before starting producers or consumers.
2. Always verify environment variables in the `.env` file.
3. Regularly check logs in `logs/rafting_project_log.log`.

---

## üìù License

This project is licensed under the **MIT License**. You are encouraged to fork, modify, and explore the code.

[![Python Version](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/) 

[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)  

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Jason%20A.%20Ballard-blue?style=flat&logo=linkedin)](https://www.linkedin.com/in/jasonaballard/)  

---
_Project completed Februray 16th 2025_
---

